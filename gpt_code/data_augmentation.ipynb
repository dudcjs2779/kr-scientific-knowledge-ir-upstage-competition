{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.26.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/rag_env/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/envs/rag_env/lib/python3.10/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/rag_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/envs/rag_env/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/rag_env/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.26.0-py3-none-any.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sniffio, pydantic-core, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.26.0 pydantic-2.7.1 pydantic-core-2.18.2 sniffio-1.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-dotenv in /opt/conda/envs/rag_env/lib/python3.10/site-packages (1.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from utils import file_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword 생성\n",
    "- gpt3.5로 [문서, 질의] pair 데이터를 생성하기 이전에 키워드 생성해 더 다양한 질의를 생성할 수 있도록 유도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "# gpt-4-turbo\n",
    "# gpt-3.5-turbo-0125\n",
    "llm_model = \"gpt-3.5-turbo-0125\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "docs_df = file_control.read_jsonl(\"data/documents.jsonl\", to_csv=True)\n",
    "docs_df['index'] = docs_df.index\n",
    "\n",
    "docs_df.to_json(\"data/documents_index.jsonl\", orient='records', lines=True, force_ascii=False)\n",
    "docs_df = file_control.read_jsonl(\"data/documents_index.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2541\n",
      "## Role\n",
      "키워드 생성기\n",
      "\n",
      "## Instructions\n",
      "- 주어진 내용을 보고 중요한 키워드만 추출하거나 생성한다.\n",
      "- 내용에는 없지만 관련성이 높은 키워드를 생성한다.\n",
      "- 중요한 키워드를 앞쪽에 배치한다.\n",
      "- JSON 포맷으로 키워드를 생성한다.\n",
      "\n",
      "## Content\n",
      "식물 세포와 동물 세포는 구조적으로 차이가 있습니다. 식물 세포에는 동물 세포에는 없는 세포 부분이 있습니다. 이 부분은 세포 벽입니다. 세포 벽은 식물 세포의 외부를 둘러싸고 있는 견고한 구조물로, 세포의 형태를 유지하고 보호하는 역할을 합니다. 세포 벽은 주로 섬유소로 이루어져 있으며, 식물 세포의 크기와 형태를 결정하는 중요한 역할을 합니다. 동물 세포에는 세포 벽이 없기 때문에, 동물 세포는 보다 유연하고 다양한 형태를 가질 수 있습니다.\n",
      "\n",
      "## Output format\n",
      "{\"keywords\": [$word1, $word2, $word3, ...]}\n"
     ]
    }
   ],
   "source": [
    "# 샘플 프롬프트\n",
    "ran = random.randint(0, len(docs_df)-1)\n",
    "# ran = 0\n",
    "print(ran)\n",
    "\n",
    "augment_instruct_test = \"\"\"\n",
    "## Role\n",
    "키워드 생성기\n",
    "\n",
    "## Instructions\n",
    "- 주어진 내용을 보고 중요한 키워드만 추출하거나 생성한다.\n",
    "- 내용에는 없지만 관련성이 높은 키워드를 생성한다.\n",
    "- 중요한 키워드를 앞쪽에 배치한다.\n",
    "- JSON 포맷으로 키워드를 생성한다.\n",
    "\n",
    "## Content\n",
    "%s\n",
    "\n",
    "## Output format\n",
    "{\"keywords\": [$word1, $word2, $word3, ...]}\n",
    "\n",
    "\"\"\" % (docs_df[ran][\"content\"])\n",
    "\n",
    "print(augment_instruct_test.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 호출 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Role\n",
      "키워드 생성기\n",
      "\n",
      "## Instructions\n",
      "- 주어진 내용을 보고 중요한 키워드만 추출하거나 생성한다.\n",
      "- 내용에는 없지만 관련성이 높은 키워드를 생성한다.\n",
      "- 중요한 키워드를 앞쪽에 배치한다.\n",
      "- JSON 포맷으로 키워드를 생성한다.\n",
      "\n",
      "## Output format\n",
      "{\"keywords\": [$word1, $word2, $word3, ...]}\n"
     ]
    }
   ],
   "source": [
    "augment_instruct = \"\"\"\n",
    "## Role\n",
    "키워드 생성기\n",
    "\n",
    "## Instructions\n",
    "- 주어진 내용을 보고 중요한 키워드만 추출하거나 생성한다.\n",
    "- 내용에는 없지만 관련성이 높은 키워드를 생성한다.\n",
    "- 중요한 키워드를 앞쪽에 배치한다.\n",
    "- JSON 포맷으로 키워드를 생성한다.\n",
    "\n",
    "## Output format\n",
    "{\"keywords\": [$word1, $word2, $word3, ...]}\n",
    "\n",
    "\"\"\"\n",
    "augment_instruct = augment_instruct.strip()\n",
    "\n",
    "print(augment_instruct.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '## Role\\n키워드 생성기\\n\\n## Instructions\\n- 주어진 내용을 보고 중요한 키워드만 추출하거나 생성한다.\\n- 내용에는 없지만 관련성이 높은 키워드를 생성한다.\\n- 중요한 키워드를 앞쪽에 배치한다.\\n- JSON 포맷으로 키워드를 생성한다.\\n\\n## Output format\\n{\"keywords\": [$word1, $word2, $word3, ...]}'},\n",
       " {'role': 'user',\n",
       "  'content': '인(원자 번호 15)은 주기율표에서 3주기 5족에 위치하고 있습니다. 이 위치를 기준으로 인은 5개의 원자가 전자를 가지고 있습니다. 이는 인의 전자 구성에 의해 결정됩니다. 인은 1s2 2s2 2p6 3s2 3p3의 전자 구성을 가지고 있으며, 이는 총 15개의 전자로 구성되어 있습니다. 이 중에서 외부 전자 껍질에는 5개의 전자가 존재하므로, 인은 5개의 원자가 전자를 가지고 있습니다.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": augment_instruct},\n",
    "        {\"role\": \"user\", \"content\": docs_df[ran][\"content\"]}\n",
    "    ]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keywords': ['인', '주기율표', '3주기', '5족', '원자 번호 15', '전자 구성', '외부 전자 껍질', '전자']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            seed=42\n",
    ")\n",
    "json.loads(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 호출 자동화 및 데이터 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_keywords(row_data):\n",
    "    data = {\n",
    "        \"index\": row_data['index'],\n",
    "        \"docid\": row_data['docid'],\n",
    "        \"content\": row_data['content'],\n",
    "        \"keywords\": []\n",
    "    }\n",
    "    \n",
    "    # print(data)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": augment_instruct},\n",
    "        {\"role\": \"user\", \"content\": data['content']}\n",
    "    ]\n",
    "    \n",
    "    # print(messages)\n",
    "    \n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=messages,\n",
    "                    temperature=0.7,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    timeout=10,\n",
    "                    seed=42\n",
    "        )\n",
    "        data['keywords'] = json.loads(result.choices[0].message.content)['keywords']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(data)\n",
    "    \n",
    "    with open(f\"data/gpt_data/keywords/doc_keyword{data['index']}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# ========= API 호출 주의 =========\n",
    "# 키워드 생성(멀티쓰레드)\n",
    "with concurrent.futures.ThreadPoolExecutor(4) as executor:\n",
    "    new_dataset = list(tqdm(executor.map(get_keywords, docs_df[0:10]), total=len(docs_df[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 병합 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ========= 덮어쓰기 주의 =========\n",
    "# 경로의 모든 jsonl 데이터를 합친후 저장\n",
    "file_control.conbine_json(\"data/gpt_data/keywords\", \"data/gpt_data/result/documents_keywords_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# index 순서 맞춰서 다시 저장\n",
    "keyword_df = file_control.read_jsonl(\"data/gpt_data/result/documents_keywords_test.jsonl\", True)\n",
    "keyword_df = keyword_df.sort_values(by='index').reset_index(drop=True)\n",
    "keyword_df.to_json(\"data/gpt_data/result/documents_keywords_test.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 생성\n",
    "- 생성한 키워드와 문서내용을 바탕으로 질의를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "# gpt-4-turbo\n",
    "# gpt-3.5-turbo-0125\n",
    "llm_model = \"gpt-3.5-turbo-0125\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# 생성한 키워드 불러오기\n",
    "documents_keywords = file_control.read_jsonl(\"data/gpt_data/result/documents_keywords.jsonl\", to_csv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2373\n",
      "\n",
      "## Role\n",
      "질문 생성기\n",
      "\n",
      "## Instructions\n",
      "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
      "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
      "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
      "- 최대한 다양한 질문을 생성해줘.\n",
      "- 질문은 한문장으로 간결하게 구성해줘.\n",
      "- 한국어로 질문을 생성해줘.\n",
      "- 아래 JSON 포맷으로 생성해줘.\n",
      "\n",
      "## Keywords\n",
      "['환형동물', '절지동물', '마디', '신체 구조', '동물', '원형 몸', '다리', '감각기관', '먹이 섭취', '번식', '근육', '신경계', '지렁이', '지하 생활', '해양 생활', '관련성']\n",
      "\n",
      "## Content\n",
      "환형동물과 절지동물은 서로 유사한 점이 있는데 두 문의 구성원들이 마디로 나뉘어진 신체를 가지고 있다는 것이다. 환형동물은 동물의 한 종류로, 그들은 몸이 원형이며 여러 개의 마디로 이루어져 있다. 예를 들어, 지네와 같은 작은 환형동물은 몸이 여러 개의 마디로 나뉘어져 있으며, 각 마디에는 다리와 감각기관이 있다. 또한, 환형동물은 먹이를 섭취하고 번식하는 데에도 마디를 사용한다. 절지동물은 또 다른 동물의 종류로, 그들도 마디로 나뉘어진 신체를 가지고 있다. 예를 들어, 지렁이는 몸이 여러 개의 마디로 이루어져 있으며, 각 마디에는 근육과 신경계가 있다. 절지동물은 지렁이처럼 지하에서 생활하거나, 해양에서 수중 생활을 할 수도 있다. 환형동물과 절지동물은 이러한 공통점을 가지고 있으며, 이러한 특징은 그들이 서로 관련이 있다는 것을 시사한다.\n",
      "\n",
      "## Output format\n",
      "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 샘플 프롬프트\n",
    "ran = random.randint(0, len(documents_keywords)-1)\n",
    "# ran=3571\n",
    "print(ran)\n",
    "\n",
    "augment_instruct_test = \"\"\"\n",
    "## Role\n",
    "질문 생성기\n",
    "\n",
    "## Instructions\n",
    "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
    "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
    "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
    "- 최대한 다양한 질문을 생성해줘.\n",
    "- 질문은 한문장으로 간결하게 구성해줘.\n",
    "- 한국어로 질문을 생성해줘.\n",
    "- 아래 JSON 포맷으로 생성해줘.\n",
    "\n",
    "## Keywords\n",
    "%s\n",
    "\n",
    "## Content\n",
    "%s\n",
    "\n",
    "## Output format\n",
    "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
    "\"\"\" % (documents_keywords[ran]['keywords'], documents_keywords[ran]['content'])\n",
    "\n",
    "print(augment_instruct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 호출 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507\n",
      "\n",
      "## Role\n",
      "질문 생성기\n",
      "\n",
      "## Instructions\n",
      "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
      "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
      "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
      "- 최대한 다양한 질문을 생성해줘.\n",
      "- 질문은 한문장으로 간결하게 구성해줘.\n",
      "- 한국어로 질문을 생성해줘.\n",
      "- 아래 JSON 포맷으로 생성해줘.\n",
      "\n",
      "## Output format\n",
      "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
      "\n",
      "\n",
      "## Keywords\n",
      "['염수', '담수', '해안', '염분', '밀도', '퍼져나가는 과정', '영양분', '해안 생태계']\n",
      "\n",
      "## Content\n",
      "담수와 염수가 강어귀에서 만나면, 염수는 일반적으로 담수 아래로 흐르는데 그 이유는 염수가 밀도가 더 높기 때문입니다. 이러한 현상은 해안에서 퍼져나가는 과정을 통해 발생합니다. 해안에서 염수가 담수와 만나면, 염수는 담수보다 더 높은 염분 농도를 가지고 있기 때문에 담수 아래로 흐르게 됩니다. 이러한 현상은 담수에 농축된 영양분에도 영향을 미칩니다. 염수가 담수로 흐르면서 담수에 농축된 영양분도 함께 퍼져나가게 되어, 해안 생태계에 영양분을 공급하게 됩니다. 따라서, 염수가 해안에서 퍼져나가는 것이 담수에 농축된 영양분에 가장 가능성이 높은 영향을 미치는 것으로 생각됩니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ran = random.randint(0, len(documents_keywords)-1)\n",
    "# ran = 0\n",
    "print(ran)\n",
    "\n",
    "augment_instruct = \"\"\"\n",
    "## Role\n",
    "질문 생성기\n",
    "\n",
    "## Instructions\n",
    "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
    "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
    "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
    "- 최대한 다양한 질문을 생성해줘.\n",
    "- 질문은 한문장으로 간결하게 구성해줘.\n",
    "- 한국어로 질문을 생성해줘.\n",
    "- 아래 JSON 포맷으로 생성해줘.\n",
    "\n",
    "## Output format\n",
    "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "user_input = \"\"\"\n",
    "## Keywords\n",
    "%s\n",
    "\n",
    "## Content\n",
    "%s\n",
    "\"\"\"% (documents_keywords[ran]['keywords'], documents_keywords[ran]['content'])\n",
    "\n",
    "\n",
    "print(augment_instruct)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\n## Role\\n질문 생성기\\n\\n## Instructions\\n- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\\n- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\\n- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\\n- 최대한 다양한 질문을 생성해줘.\\n- 질문은 한문장으로 간결하게 구성해줘.\\n- 한국어로 질문을 생성해줘.\\n- 아래 JSON 포맷으로 생성해줘.\\n\\n## Output format\\n{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\\n'},\n",
       " {'role': 'user',\n",
       "  'content': \"\\n## Keywords\\n['염수', '담수', '해안', '염분', '밀도', '퍼져나가는 과정', '영양분', '해안 생태계']\\n\\n## Content\\n담수와 염수가 강어귀에서 만나면, 염수는 일반적으로 담수 아래로 흐르는데 그 이유는 염수가 밀도가 더 높기 때문입니다. 이러한 현상은 해안에서 퍼져나가는 과정을 통해 발생합니다. 해안에서 염수가 담수와 만나면, 염수는 담수보다 더 높은 염분 농도를 가지고 있기 때문에 담수 아래로 흐르게 됩니다. 이러한 현상은 담수에 농축된 영양분에도 영향을 미칩니다. 염수가 담수로 흐르면서 담수에 농축된 영양분도 함께 퍼져나가게 되어, 해안 생태계에 영양분을 공급하게 됩니다. 따라서, 염수가 해안에서 퍼져나가는 것이 담수에 농축된 영양분에 가장 가능성이 높은 영향을 미치는 것으로 생각됩니다.\\n\"}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": augment_instruct},\n",
    "    {\"role\": \"user\", \"content\": user_input}\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': ['염수와 담수가 만나는 곳에서 염수가 일반적으로 어떤 방향으로 흐르는가?',\n",
       "  '해안에서 염수와 담수가 만나면 염수가 담수보다 더 높은 무엇을 가지고 있는가?',\n",
       "  '해안에서 염수가 담수로 흐르면 어떤 현상이 발생하며 이는 어떠한 영향을 미치는가?',\n",
       "  '염수가 담수에 농축된 영양분을 함께 퍼져나가게 되는 과정은 어디에서 발생하는가?',\n",
       "  '해안 생태계에 영양분을 공급하는 과정은 무엇에 의해 이루어지는가?']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            seed=42\n",
    ")\n",
    "json.loads(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API 호출 자동화 및 데이터 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Role\n",
      "질문 생성기\n",
      "\n",
      "## Instructions\n",
      "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
      "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
      "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
      "- 최대한 다양한 질문을 생성해줘.\n",
      "- 질문은 한문장으로 간결하게 구성해줘.\n",
      "- 한국어로 질문을 생성해줘.\n",
      "- 아래 JSON 포맷으로 생성해줘.\n",
      "\n",
      "## Output format\n",
      "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
      "\n",
      "\n",
      "## Keywords\n",
      "['물을 이용한 전기 생산', '수력 발전소', '지역 생태계', '환경적인 측면', '생태계의 균형', '수력 발전소 건설', '물의 흐름 변화', '수중 생태계 영향', '자연 환경 희생']\n",
      "\n",
      "## Content\n",
      "움직이는 물을 이용해 전기를 생산하는 것은 많은 이점이 있지만, 단점도 존재합니다. 그 중 하나는 지역 생태계가 혼란될 수 있다는 것입니다. 물을 이용한 전기 생산은 보통 대규모 수력 발전소에서 이루어지는데, 이로 인해 물의 흐름이 변화하고 수중 생태계에 영향을 줄 수 있습니다. 또한, 수력 발전소를 건설하기 위해 수많은 토지와 자연 환경을 희생해야 할 수도 있습니다. 이는 지역 생태계에 큰 영향을 미칠 수 있으며, 생태계의 균형을 깨뜨릴 수도 있습니다. 따라서, 움직이는 물을 이용해 전기를 생산하는 것은 환경적인 측면에서 신중한 고려가 필요한 분야입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 작성\n",
    "ran = random.randint(0, len(documents_keywords)-1)\n",
    "ran = 0\n",
    "\n",
    "# == 지시사항 ==\n",
    "augment_instruct = \"\"\"\n",
    "## Role\n",
    "질문 생성기\n",
    "\n",
    "## Instructions\n",
    "- 주어진 키워드를 참고하여 질문을 5개 생성해줘.\n",
    "- 주어진 레퍼런스 정보를 보고 이 정보가 도움이 될만한 질문을 생성해줘.\n",
    "- 앞쪽에 위치한 키워드들은 반드시 포함하여 질문을 생성해줘.\n",
    "- 최대한 다양한 질문을 생성해줘.\n",
    "- 질문은 한문장으로 간결하게 구성해줘.\n",
    "- 한국어로 질문을 생성해줘.\n",
    "- 아래 JSON 포맷으로 생성해줘.\n",
    "\n",
    "## Output format\n",
    "{\"questions\": [$question1, $question2, $question3, $question4, $question5]}\n",
    "\"\"\" \n",
    "\n",
    "# == 모델 입력 ==\n",
    "user_input = \"\"\"\n",
    "## Keywords\n",
    "%s\n",
    "\n",
    "## Content\n",
    "%s\n",
    "\"\"\"% (documents_keywords[50]['keywords'], documents_keywords[50]['content'])\n",
    "\n",
    "print(augment_instruct)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_questions(row_data):\n",
    "    global start_point\n",
    "    data = {\n",
    "        \"index\": row_data['index'],\n",
    "        \"docid\": row_data['docid'],\n",
    "        \"content\": row_data['content'],\n",
    "        \"keywords\": row_data['keywords'],\n",
    "        \"questions\": []\n",
    "    }\n",
    "    # print(data)\n",
    "    \n",
    "    # 모델 입력\n",
    "    user_input = \"\"\"\n",
    "## Keywords\n",
    "%s\n",
    "\n",
    "## Content\n",
    "%s\n",
    "\"\"\"% (data['keywords'], data['content'])\n",
    "\n",
    "    # print(augment_instruct)\n",
    "    # print(user_input)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": augment_instruct},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    # print(messages)\n",
    "    \n",
    "    try:\n",
    "        result = client.chat.completions.create(\n",
    "                    model=llm_model,\n",
    "                    messages=messages,\n",
    "                    temperature=1.0,\n",
    "                    response_format={\"type\": \"json_object\"},\n",
    "                    seed=42\n",
    "        )\n",
    "        data['questions'] = json.loads(result.choices[0].message.content)['questions']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(data)\n",
    "    \n",
    "    # 파일 저장\n",
    "    with open(f\"data/gpt_data/questions/doc_questions{data['index']}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# 멀티스레딩 호출\n",
    "with concurrent.futures.ThreadPoolExecutor(4) as executor:\n",
    "    new_dataset = list(tqdm(executor.map(get_questions, documents_keywords[0:10]), total=len(documents_keywords[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ========= 덮어쓰기 주의 =========\n",
    "# 경로의 모든 jsonl 데이터를 합친후 저장\n",
    "file_control.conbine_json(\"data/gpt_data/questions\", \"data/gpt_data/result/documents_questions_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병합 결과물 중 누락된 데이터 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "doc_questions = file_control.read_jsonl(\"data/gpt_data/result/documents_questions_test.jsonl\", True)\n",
    "doc_questions_df = pd.DataFrame(doc_questions).sort_values(by='index').reset_index(drop=True)\n",
    "index_list = doc_questions_df['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앞뒤 row 데이터의 index 값이 1이상 차이나는 데이터 찾기\n",
    "missing_index = []\n",
    "for i in range(0, len(index_list)-1):\n",
    "    if index_list[i+1] - index_list[i] !=1:\n",
    "         print(index_list[i]+1)\n",
    "         missing_index.append(index_list[i]+1)\n",
    "         \n",
    "missing_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "documents_keywords_df = file_control.read_jsonl(\"data/gpt_data/result/documents_keywords_test.jsonl\", to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>docid</th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, docid, content, keywords]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 누락된 데이터 확인\n",
    "documents_keywords_df[documents_keywords_df['index'].isin(missing_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "missing_df = documents_keywords_df[documents_keywords_df['index'].isin(missing_index)]\n",
    "missings = missing_df.to_json(orient='records', lines=False, force_ascii=False)\n",
    "missings = json.loads(missings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 누락된 데이터 재호출\n",
    "with concurrent.futures.ThreadPoolExecutor(4) as executor:\n",
    "    new_dataset = list(tqdm(executor.map(get_questions, missings), total=len(missings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ========= 덮어쓰기 주의 =========\n",
    "# 경로의 모든 jsonl 데이터를 합친후 저장\n",
    "file_control.conbine_json(\"data/gpt_data/questions\", \"data/gpt_data/result/documents_questions_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# index 순서 맞춰서 다시 저장\n",
    "questions_df = file_control.read_jsonl(\"data/gpt_data/result/documents_questions_test.jsonl\", True)\n",
    "questions_df = questions_df.sort_values(by='index').reset_index(drop=True)0\n",
    "questions_df.to_json(\"data/gpt_data/result/documents_keywords_test.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "col_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
